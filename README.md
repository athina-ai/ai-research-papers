# ai-research-papers
Summaries of AI Research Papers

## Prompt Engineering
- Reflexion: Language Agents with Verbal Reinforcement Learning ([summary](https://blog.athina.ai/reflexion-language-agents-with-verbal-reinforcement-learning) | [paper](https://arxiv.org/pdf/2303.11366.pdf))
- A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning Program Repair ([summary](https://blog.athina.ai/a-study-on-prompt-design-advantages-and-limitations-of-chatgpt-for-deep-learning-program-repair) | [paper](https://arxiv.org/abs/2304.0819))
- Prompt-Free Diffusion: Taking 'Text' out of Text-to-Image Diffusion Models ([summary](https://blog.athina.ai/prompt-free-diffusion-taking-text-out-of-text-to-image-diffusion-models) | [paper](https://arxiv.org/abs/2305.16223))
- How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings ([summary](https://blog.athina.ai/how-to-prompt-llms-for-text-to-sql-a-study-in-zero-shot-single-domain-and-cross-domain-settings) | [paper](https://arxiv.org/abs/2305.11853))
- Exploring EFL students' prompt engineering in human-AI story writing: an Activity Theory perspective ([summary](https://blog.athina.ai/exploring-efl-students-prompt-engineering-in-human-ai-story-writing-an-activity-theory-perspective) | [paper](https://arxiv.org/abs/2306.01798))
- A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications ([summary](https://blog.athina.ai/a-systematic-survey-of-prompt-engineering-in-large-language-models-techniques-and-applications) | [paper](https://arxiv.org/abs/2402.07927))
- Wordflow: Social Prompt Engineering for Large Language Models ([summary](https://blog.athina.ai/wordflow-social-prompt-engineering-for-large-language-models) | [paper](https://arxiv.org/abs/2401.14447))
- Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation ([summary](https://blog.athina.ai/automated-black-box-prompt-engineering-for-personalized-text-to-image-generation) | [paper](https://arxiv.org/abs/2403.1910))
- Exploring Prompt Engineering Practices in the Enterprise ([summary](https://blog.athina.ai/exploring-prompt-engineering-practices-in-the-enterprise) | [paper](https://arxiv.org/abs/2403.0895))
- ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level Generation ([summary](https://blog.athina.ai/chatgpt4pcg-2-competition-prompt-engineering-for-science-birds-level-generation) | [paper](https://arxiv.org/abs/2403.02610))
- MedPromptExtract (Medical Data Extraction Tool): Anonymization and Hi-fidelity Automated data extraction using NLP and prompt engineering ([summary](https://blog.athina.ai/medpromptextract-medical-data-extraction-tool-anonymization-and-hi-fidelity-automated-data-extraction-using-nlp-and-prompt-engineering) | [paper](https://arxiv.org/abs/2405.02664))
- Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering ([summary](https://blog.athina.ai/exploring-the-intersection-of-large-language-models-and-agent-based-modeling-via-prompt-engineering) | [paper](https://arxiv.org/abs/2308.07411))
- Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation ([summary](https://blog.athina.ai/large-language-models-and-prompt-engineering-for-biomedical-query-focused-multi-document-summarisation) | [paper](https://arxiv.org/abs/2312.04344))
- Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies ([summary](https://blog.athina.ai/enhancing-medical-task-performance-in-gpt-4v-a-comprehensive-study-on-prompt-engineering-strategies) | [paper](https://arxiv.org/abs/2312.04344))
- Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT ([summary](https://blog.athina.ai/cases-of-efl-secondary-students-prompt-engineering-pathways-to-complete-a-writing-task-with-chatgpt) | [paper](https://arxiv.org/abs/2307.05493))
- Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4 ([summary](https://blog.athina.ai/prompt-engineering-assisted-malware-dynamic-analysis-using-gpt-4) | [paper](https://arxiv.org/abs/2312.08317))
- Prompt-Engineering and Transformer-based Question Generation and Evaluation ([summary](https://blog.athina.ai/prompt-engineering-and-transformer-based-question-generation-and-evaluation) | [paper](https://arxiv.org/abs/2310.18867))
- PEACE: Prompt Engineering Automation for CLIPSeg Enhancement in Aerial Robotics ([summary](https://blog.athina.ai/peace-prompt-engineering-automation-for-clipseg-enhancement-in-aerial-robotics) | [paper](https://arxiv.org/abs/2310.000))
- To be or not to be? an exploration of continuously controllable prompt engineering ([summary](https://blog.athina.ai/to-be-or-not-to-be-an-exploration-of-continuously-controllable-prompt-engineering) | [paper](https://arxiv.org/abs/2311.09773))
- A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models ([summary](https://blog.athina.ai/a-systematic-survey-of-prompt-engineering-on-vision-language-foundation-models) | [paper](https://arxiv.org/abs/2307.12980))
- Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering ([summary](https://blog.athina.ai/prompting-ai-art-an-investigation-into-the-creative-skill-of-prompt-engineering) | [paper](https://arxiv.org/abs/2303.13534))
- Prompt Engineering a Prompt Engineer ([summary](https://blog.athina.ai/prompt-engineering-a-prompt-engineer) | [paper](https://arxiv.org/abs/2311.05661))
- Prompt Engineering for Healthcare: Methodologies and Applications ([summary](https://blog.athina.ai/prompt-engineering-for-healthcare-methodologies-and-applications) | [paper](https://arxiv.org/abs/2304.14670))
- Large Language Models Are Human-Level Prompt Engineers ([summary](https://blog.athina.ai/large-language-models-are-human-level-prompt-engineers) | [paper](https://arxiv.org/abs/2211.01910))
- DocPrompting: Generating Code by Retrieving the Docs ([summary](https://blog.athina.ai/docprompting-generating-code-by-retrieving-the-docs) | [paper](https://arxiv.org/abs/2207.05987))
- PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization ([summary](https://blog.athina.ai/promptagent-strategic-planning-with-language-models-enables-expert-level-prompt-optimization) | [paper](https://arxiv.org/abs/2310.16427))
- LLM Guided Evolution -- The Automation of Models Advancing Models ([summary](https://blog.athina.ai/llm-guided-evolution----the-automation-of-models-advancing-models) | [paper](https://arxiv.org/abs/2403.11446))
- Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review ([summary](https://blog.athina.ai/unleashing-the-potential-of-prompt-engineering-in-large-language-models-a-comprehensive-review) | [paper](https://arxiv.org/pdf/2310.14735))
- Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery ([summary](https://blog.athina.ai/hard-prompts-made-easy-gradient-based-discrete-optimization-for-prompt-tuning-and-discovery) | [paper](https://arxiv.org/abs/2302.03668))
- Compositional Exemplars for In-context Learning ([summary](https://blog.athina.ai/compositional-exemplars-for-in-context-learning) | [paper](https://arxiv.org/abs/2302.05698))
- SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains ([summary](https://blog.athina.ai/switchprompt-learning-domain-specific-gated-soft-prompts-for-classification-in-low-resource-domains) | [paper](https://arxiv.org/abs/2302.06868))
- GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks ([summary](https://blog.athina.ai/graphprompt-unifying-pre-training-and-downstream-tasks-for-graph-neural-networks) | [paper](https://arxiv.org/abs/2302.08043))
- Scalable Prompt Generation for Semi-supervised Learning with Language Models ([summary](https://blog.athina.ai/scalable-prompt-generation-for-semi-supervised-learning-with-language-models) | [paper](https://arxiv.org/abs/2302.09236))
- Guiding Large Language Models via Directional Stimulus Prompting ([summary](https://blog.athina.ai/guiding-large-language-models-via-directional-stimulus-prompting) | [paper](https://arxiv.org/abs/2302.11520))
- A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT ([summary](https://blog.athina.ai/a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgpt) | [paper](https://arxiv.org/abs/2302.11382))
- EvoPrompting: Language Models for Code-Level Neural Architecture Search ([summary](https://blog.athina.ai/evoprompting-language-models-for-code-level-neural-architecture-search) | [paper](https://arxiv.org/abs/2302.14838))
- Dynamic Prompting: A Unified Framework for Prompt Tuning ([summary](https://blog.athina.ai/dynamic-prompting-a-unified-framework-for-prompt-tuning) | [paper](https://arxiv.org/abs/2303.02909))
- CAMEL: Communicative Agents for 'Mind' Exploration of Large Language Model Society ([summary](https://blog.athina.ai/camel-communicative-agents-for-mind-exploration-of-large-language-model-society) | [paper](https://arxiv.org/abs/2303.17760))
- Boosted Prompt Ensembles for Large Language Models ([summary](https://blog.athina.ai/boosted-prompt-ensembles-for-large-language-models) | [paper](https://arxiv.org/abs/2304.05970))
- Efficient Prompting via Dynamic In-Context Learning ([summary](https://blog.athina.ai/efficient-prompting-via-dynamic-in-context-learning) | [paper](https://arxiv.org/abs/2305.11170))
- Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt ([summary](https://blog.athina.ai/compress-then-prompt-improving-accuracy-efficiency-trade-off-of-llm-inference-with-transferable-prompt) | [paper](https://arxiv.org/abs/2305.11186))
- Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models ([summary](https://blog.athina.ai/plan-and-solve-prompting-improving-zero-shot-chain-of-thought-reasoning-by-large-language-models) | [paper](https://arxiv.org/abs/2305.04091v3))
- Hierarchical Prompting Assists Large Language Model on Web Navigation ([summary](https://blog.athina.ai/hierarchical-prompting-assists-large-language-model-on-web-navigation) | [paper](https://arxiv.org/abs/2305.14257))
- Better Zero-Shot Reasoning with Self-Adaptive Prompting ([summary](https://blog.athina.ai/better-zero-shot-reasoning-with-self-adaptive-prompting) | [paper](https://arxiv.org/abs/2305.14106))
- PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents ([summary](https://blog.athina.ai/pearl-prompting-large-language-models-to-plan-and-execute-actions-over-long-documents) | [paper](https://arxiv.org/abs/2305.14564v1))
- Exploring Lottery Prompts for Pre-trained Language Models ([summary](https://blog.athina.ai/exploring-lottery-prompts-for-pre-trained-language-models) | [paper](https://arxiv.org/abs/2305.19500))
- Graph of Thoughts: Solving Elaborate Problems with Large Language Models ([summary](https://blog.athina.ai/graph-of-thoughts-solving-elaborate-problems-with-large-language-models) | [paper](https://arxiv.org/abs/2308.09687v2))
- From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting ([summary](https://blog.athina.ai/from-sparse-to-dense-gpt-4-summarization-with-chain-of-density-prompting) | [paper](https://arxiv.org/abs/2309.04269))
- Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing ([summary](https://blog.athina.ai/pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processing) | [paper](https://arxiv.org/abs/2107.13586))
- A Taxonomy of Prompt Modifiers for Text-To-Image Generation ([summary](https://blog.athina.ai/a-taxonomy-of-prompt-modifiers-for-text-to-image-generation) | [paper](https://arxiv.org/abs/2204.13988))
- Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study ([summary](https://blog.athina.ai/jailbreaking-chatgpt-via-prompt-engineering-an-empirical-study) | [paper](https://arxiv.org/abs/2305.13860))
- Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data ([summary](https://blog.athina.ai/automatic-prompt-augmentation-and-selection-with-chain-of-thought-from-labeled-data) | [paper](https://arxiv.org/abs/2302.12822))
- Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following ([summary](https://blog.athina.ai/investigating-the-effectiveness-of-task-agnostic-prefix-prompt-for-instruction-following) | [paper](https://arxiv.org/abs/2302.14691))
- Model-tuning Via Prompts Makes NLP Models Adversarially Robust ([summary](https://blog.athina.ai/model-tuning-via-prompts-makes-nlp-models-adversarially-robust) | [paper](https://arxiv.org/abs/2303.07320))
- UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation ([summary](https://blog.athina.ai/uprise-universal-prompt-retrieval-for-improving-zero-shot-evaluation) | [paper](https://arxiv.org/abs/2303.08518))
- A Comprehensive Survey on Instruction Following ([summary](https://blog.athina.ai/uprise-universal-prompt-retrieval-for-improving-zero-shot-evaluation) | [paper](https://arxiv.org/abs/2303.10475))
- Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models ([summary](https://blog.athina.ai/knowledge-card-filling-llms-knowledge-gaps-with-plug-in-specialized-language-models) | [paper](https://arxiv.org/abs/2305.09955))
- Tree of Thoughts: Deliberate Problem Solving with Large Language Models ([summary](https://blog.athina.ai/tree-of-thoughts-deliberate-problem-solving-with-large-language-models) | [paper](https://arxiv.org/abs/2305.10601))
- Post Hoc Explanations of Language Models Can Improve Language Models ([summary](https://blog.athina.ai/post-hoc-explanations-of-language-models-can-improve-language-models) | [paper](https://arxiv.org/abs/2305.11426))
- Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting ([summary](https://blog.athina.ai/enhancing-large-language-models-against-inductive-instructions-with-dual-critique-prompting) | [paper](https://arxiv.org/abs/2305.13733))
- Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation ([summary](https://blog.athina.ai/skeleton-of-thought-prompting-llms-for-efficient-parallel-generation) | [paper](https://arxiv.org/abs/2307.15337))
- Re-Reading Improves Reasoning in Large Language Models ([summary](https://blog.athina.ai/re-reading-improves-reasoning-in-large-language-models) | [paper](https://arxiv.org/abs/2309.06275))
- Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers ([summary](https://blog.athina.ai/connecting-large-language-models-with-evolutionary-algorithms-yields-powerful-prompt-optimizers) | [paper](https://arxiv.org/abs/2309.08532))
- LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models ([summary](https://blog.athina.ai/llmlingua-compressing-prompts-for-accelerated-inference-of-large-language-models) | [paper](https://arxiv.org/abs/2310.05736))
- Large Language Models as Analogical Reasoners ([summary](https://blog.athina.ai/large-language-models-as-analogical-reasoners) | [paper](https://arxiv.org/abs/2310.01714))
- Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4 ([summary](https://blog.athina.ai/principled-instructions-are-all-you-need-for-questioning-llama-1/2-gpt-3.5/4) | [paper](https://arxiv.org/abs/2312.16171v1))
- Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic ([summary](https://blog.athina.ai/enhancing-zero-shot-chain-of-thought-reasoning-in-large-language-models-through-logic) | [paper](https://arxiv.org/abs/2309.13339))
- Prompt Design and Engineering: Introduction and Advanced Methods ([summary](https://blog.athina.ai/prompt-design-and-engineering-introduction-and-advanced-methods) | [paper](https://arxiv.org/abs/2401.14423))
- Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions ([summary](https://blog.athina.ai/probabilistic-tree-of-thought-reasoning-for-answering-knowledge-intensive-complex-questions) | [paper](https://arxiv.org/abs/2311.13982))
- Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks ([summary](https://blog.athina.ai/search-in-the-chain-interactively-enhancing-large-language-models-with-search-for-knowledge-intensive-tasks) | [paper](https://arxiv.org/abs/2304.14732))
- Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources ([summary](https://blog.athina.ai/chain-of-knowledge-grounding-large-language-models-via-dynamic-knowledge-adapting-over-heterogeneous-sources) | [paper](https://arxiv.org/abs/2305.13269))
- Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning ([summary](https://blog.athina.ai/semi-structured-chain-of-thought-integrating-multiple-sources-of-knowledge-for-improved-language-model-reasoning) | [paper](https://arxiv.org/abs/2311.08505))
- EntGPT: Linking Generative Large Language Models with Knowledge Bases ([summary](https://blog.athina.ai/entgpt-linking-generative-large-language-models-with-knowledge-bases) | [paper](https://arxiv.org/abs/2402.06738))
- Chain-of-Verification Reduces Hallucination in Large Language Models ([summary](https://blog.athina.ai/chain-of-verification-reduces-hallucination-in-large-language-models) | [paper](https://arxiv.org/abs/2309.11495))
- Reflexion: Language Agents with Verbal Reinforcement Learning ([summary](https://blog.athina.ai/reflexion-language-agents-with-verbal-reinforcement-learning) | [paper](https://arxiv.org/pdf/2303.11366.pdf))


## Hallucinations
- DiffusionGPT: LLM-Driven Text-to-Image Generation System ([summary](https://blog.athina.ai/diffusiongpt-llm-driven-text-to-image-generation-system) | [paper](https://arxiv.org/abs/2401.10061))
- Chain-of-Verification Reduces Hallucination in Large Language Models ([summary](https://blog.athina.ai/chain-of-verification-reduces-hallucination-in-large-language-models) | [paper](https://arxiv.org/abs/2309.11495))
- A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions ([summary](https://blog.athina.ai/a-survey-on-hallucination-in-large-language-models-principles-taxonomy-challenges-and-open-questions) | [paper](https://arxiv.org/abs/2311.052))
- AutoHall: Automated Hallucination Dataset Generation for Large Language Models ([summary](https://blog.athina.ai/autohall-automated-hallucination-dataset-generation-for-large-language-models) | [paper](https://arxiv.org/abs/2310.0025))
- Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models ([summary](https://blog.athina.ai/siren-s-song-in-the-ai-ocean-a-survey-on-hallucination-in-large-language-models) | [paper](https://arxiv.org/abs/2309.01219))
- Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation ([summary](https://blog.athina.ai/self-contradictory-hallucinations-of-large-language-models-evaluation-detection-and-mitigation) | [paper](https://arxiv.org/abs/2305.15852))
- Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination ([summary](https://blog.athina.ai/deficiency-of-large-language-models-in-finance-an-empirical-examination-of-hallucination) | [paper](https://arxiv.org/abs/2311.15548))
- Factuality of Large Language Models in the Year 2024 ([summary](https://blog.athina.ai/factuality-of-large-language-models-in-the-year-2024) | [paper](https://arxiv.org/abs/2402.02420))
- Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification ([summary](https://blog.athina.ai/ever-mitigating-hallucination-in-large-language-models-through-real-time-verification-and-rectification) | [paper](https://arxiv.org/html/2311.09114v2))

## RAG
- Chit-Chat or Deep Talk: Prompt Engineering for Process Mining ([summary](https://blog.athina.ai/chit-chat-or-deep-talk-prompt-engineering-for-process-mining) | [paper](https://arxiv.org/abs/2307.09909))
- Prompt Engineering for Transformer-based Chemical Similarity Search Identifies Structurally Distinct Functional Analogues ([summary](https://blog.athina.ai/prompt-engineering-for-transformer-based-chemical-similarity-search-identifies-structurally-distinct-functional-analogues) | [paper](https://arxiv.org/abs/2305.16330))
- Recitation-Augmented Language Models ([summary](https://blog.athina.ai/recitation-augmented-language-models) | [paper](https://arxiv.org/abs/2210.01296))
- RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval ([summary](https://blog.athina.ai/rnns-are-not-transformers-yet-the-key-bottleneck-on-in-context-retrieval) | [paper](https://arxiv.org/abs/2402.18510))
- Retrieval-Augmented Thought Process as Sequential Decision Making ([summary](https://blog.athina.ai/retrieval-augmented-thought-process-as-sequential-decision-making) | [paper](https://arxiv.org/abs/2402.07812))
- Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP ([summary](https://blog.athina.ai/demonstrate-search-predict-composing-retrieval-and-language-models-for-knowledge-intensive-nlp) | [paper](https://arxiv.org/abs/2212.14024))
- Language Is Not All You Need: Aligning Perception with Language Models ([summary](https://blog.athina.ai/language-is-not-all-you-need-aligning-perception-with-language-models) | [paper](https://arxiv.org/abs/2302.14045))
- Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer ([summary](https://blog.athina.ai/structure-pretraining-and-prompt-tuning-for-knowledge-graph-transfer) | [paper](https://arxiv.org/abs/2303.03922))
- Pre-Training to Learn in Context ([summary](https://blog.athina.ai/pre-training-to-learn-in-context) | [paper](https://arxiv.org/abs/2305.09137))
- The Web Can Be Your Oyster for Improving Large Language Models ([summary](https://blog.athina.ai/the-web-can-be-your-oyster-for-improving-large-language-models) | [paper](https://arxiv.org/abs/2305.10998))
- A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models ([summary](https://blog.athina.ai/a-step-closer-to-comprehensive-answers-constrained-multi-stage-question-decomposition-with-large-language-models) | [paper](https://arxiv.org/abs/2311.07491))
- Active Retrieval Augmented Generation ([summary](https://blog.athina.ai/active-retrieval-augmented-generation) | [paper](https://arxiv.org/abs/2305.06983))
- KnowGPT: Knowledge Injection for Large Language Models ([summary](https://blog.athina.ai/knowgpt-knowledge-injection-for-large-language-models) | [paper](https://arxiv.org/abs/2312.06185))

## Tutorial 
- SPROUT: Authoring Programming Tutorials with Interactive Visualization of Large Language Model Generation Process ([summary](https://blog.athina.ai/sprout-authoring-programming-tutorials-with-interactive-visualization-of-large-language-model-generation-process) | [paper](https://arxiv.org/abs/2312.01801))

## Dataset Generation
- Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training ([summary](https://blog.athina.ai/towards-large-scale-3d-representation-learning-with-multi-dataset-point-prompt-training) | [paper](https://arxiv.org/abs/2308.09718))
- Text2MDT: Extracting Medical Decision Trees from Medical Texts ([summary](https://blog.athina.ai/text2mdt-extracting-medical-decision-trees-from-medical-texts) | [paper](https://arxiv.org/abs/2401.02034))
- Analyzing Toxicity in Deep Conversations: A Reddit Case Study ([summary](https://blog.athina.ai/analyzing-toxicity-in-deep-conversations-a-reddit-case-study) | [paper](https://arxiv.org/abs/2404.07879))
- GuReT: Distinguishing Guilt and Regret related Text ([summary](https://blog.athina.ai/guret-distinguishing-guilt-and-regret-related-text) | [paper](https://arxiv.org/abs/2401.16541))
- Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples ([summary](https://blog.athina.ai/large-language-models-are-few-shot-generators-proposing-hybrid-prompt-algorithm-to-generate-webshell-escape-samples) | [paper](https://arxiv.org/abs/2402.07408))
- Mixture of Soft Prompts for Controllable Data Generation ([summary](https://blog.athina.ai/mixture-of-soft-prompts-for-controllable-data-generation) | [paper](https://arxiv.org/abs/2303.01580))
- WizardLM: Empowering Large Language Models to Follow Complex Instructions ([summary](https://blog.athina.ai/wizardlm-empowering-large-language-models-to-follow-complex-instructions) | [paper](https://arxiv.org/abs/2304.12244))

### Reasoning

- Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT ([summary](https://blog.athina.ai/graph-toolformer-to-empower-llms-with-graph-reasoning-ability-via-prompt-augmented-by-chatgpt) | [paper](https://arxiv.org/abs/2304.11116))
- Understanding prompt engineering may not require rethinking generalization ([summary](https://blog.athina.ai/understanding-prompt-engineering-may-not-require-rethinking-generalization) | [paper](https://arxiv.org/abs/2310.03957))
- Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought ([summary](https://blog.athina.ai/language-models-are-greedy-reasoners-a-systematic-formal-analysis-of-chain-of-thought) | [paper](https://arxiv.org/abs/2210.01240v3))
- Decomposed Prompting: A Modular Approach for Solving Complex Tasks ([summary](https://blog.athina.ai/decomposed-prompting-a-modular-approach-for-solving-complex-tasks) | [paper](https://arxiv.org/abs/2210.02406))
- ReAct: Synergizing Reasoning and Acting in Language Models ([summary](https://blog.athina.ai/react-synergizing-reasoning-and-acting-in-language-models) | [paper](https://arxiv.org/abs/2210.03629))
- PAL: Program-aided Language Models ([summary](https://blog.athina.ai/pal-program-aided-language-models) | [paper](https://arxiv.org/abs/2211.10435))
- Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning ([summary](https://blog.athina.ai/dynamic-prompt-learning-via-policy-gradient-for-semi-structured-mathematical-reasoning) | [paper](https://arxiv.org/abs/2209.14610))
- Making Large Language Models Better Reasoners with Step-Aware Verifier ([summary](https://blog.athina.ai/making-large-language-models-better-reasoners-with-step-aware-verifier) | [paper](https://arxiv.org/abs/2206.02336))
- Large Language Models are Zero-Shot Reasoners ([summary](https://blog.athina.ai/large-language-models-are-zero-shot-reasoners) | [paper](https://arxiv.org/abs/2205.11916))
- Self-Consistency Improves Chain of Thought Reasoning in Language Models ([summary](https://blog.athina.ai/self-consistency-improves-chain-of-thought-reasoning-in-language-models) | [paper](https://arxiv.org/abs/2203.11171))
- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models ([summary](https://blog.athina.ai/chain-of-thought-prompting-elicits-reasoning-in-large-language-models) | [paper](https://arxiv.org/abs/2201.11903))
- LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding ([summary](https://blog.athina.ai/layoutllm-layout-instruction-tuning-with-large-language-models-for-document-understanding) | [paper](https://arxiv.org/abs/2404.05225))
- Inferring Properties of Graph Neural Networks ([summary](https://blog.athina.ai/inferring-properties-of-graph-neural-networks) | [paper](https://arxiv.org/abs/2401.03790))
- RoT: Enhancing Large Language Models with Reflection on Search Trees ([summary](https://blog.athina.ai/rot-enhancing-large-language-models-with-reflection-on-search-trees) | [paper](https://arxiv.org/abs/2404.05449))
- STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent ([summary](https://blog.athina.ai/stamp-differentiable-task-and-motion-planning-via-stein-variational-gradient-descent) | [paper](https://arxiv.org/abs/2310.01775))
- Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting ([summary](https://blog.athina.ai/temporal-data-meets-llm----explainable-financial-time-series-forecasting) | [paper](https://arxiv.org/abs/2306.11025))
- GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations ([summary](https://blog.athina.ai/gtbench-uncovering-the-strategic-reasoning-limitations-of-llms-via-game-theoretic-evaluations) | [paper](https://arxiv.org/abs/2402.12348))
- On the Empirical Complexity of Reasoning and Planning in LLMs ([summary](https://blog.athina.ai/on-the-empirical-complexity-of-reasoning-and-planning-in-llms) | [paper](https://arxiv.org/abs/2404.11041))
- Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning ([summary](https://blog.athina.ai/evidence-to-generate-e2g-a-single-agent-two-step-prompting-for-context-grounded-and-retrieval-augmented-reasoning) | [paper](https://arxiv.org/abs/2401.05787))
- RAGAR, Your Falsehood RADAR: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models ([summary](https://blog.athina.ai/ragar-your-falsehood-radar-rag-augmented-reasoning-for-political-fact-checking-using-multimodal-large-language-models) | [paper](https://arxiv.org/abs/2404.12065))
- Chain-of-Thought Reasoning is a Policy Improvement Operator ([summary](https://blog.athina.ai/chain-of-thought-reasoning-is-a-policy-improvement-operator) | [paper](https://arxiv.org/abs/2309.08589))
- Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering ([summary](https://blog.athina.ai/tree-of-reviews-a-tree-based-dynamic-iterative-retrieval-framework-for-multi-hop-question-answering) | [paper](https://arxiv.org/abs/2404.14464))
- Autonomous Tree-search Ability of Large Language Models ([summary](https://blog.athina.ai/autonomous-tree-search-ability-of-large-language-models) | [paper](https://arxiv.org/abs/2310.10686))
- Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering ([summary](https://blog.athina.ai/knowledge-driven-cot-exploring-faithful-reasoning-in-llms-for-knowledge-intensive-question-answering) | [paper](https://arxiv.org/abs/2308.13259))
- Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation ([summary](https://blog.athina.ai/self-taught-optimizer-stop-recursively-self-improving-code-generation) | [paper](https://arxiv.org/abs/2310.02304))
- PathFinder: Guided Search over Multi-Step Reasoning Paths ([summary](https://blog.athina.ai/pathfinder-guided-search-over-multi-step-reasoning-paths) | [paper](https://arxiv.org/abs/2312.05180))
- Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models ([summary](https://blog.athina.ai/boosting-of-thoughts-trial-and-error-problem-solving-with-large-language-models) | [paper](https://arxiv.org/abs/2402.11140))
- MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems ([summary](https://blog.athina.ai/macm-utilizing-a-multi-agent-system-for-condition-mining-in-solving-complex-mathematical-problems) | [paper](https://arxiv.org/abs/2404.04735))
- Large Language Model Guided Tree-of-Thought ([summary](https://blog.athina.ai/large-language-model-guided-tree-of-thought) | [paper](https://arxiv.org/abs/2305.08291))
- Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning ([summary](https://blog.athina.ai/tree-of-mixed-thought-combining-fast-and-slow-thinking-for-multi-hop-visual-reasoning) | [paper](https://arxiv.org/abs/2308.09658))
- Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought ([summary](https://blog.athina.ai/boosting-logical-reasoning-in-large-language-models-through-a-new-framework-the-graph-of-thought) | [paper](https://arxiv.org/abs/2308.08614))
- Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts ([summary](https://blog.athina.ai/empowering-multi-step-reasoning-across-languages-via-tree-of-thoughts) | [paper](https://arxiv.org/abs/2311.08097))
- Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training ([summary](https://blog.athina.ai/alphazero-like-tree-search-can-guide-large-language-model-decoding-and-training) | [paper](https://arxiv.org/abs/2309.17179))
- Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation ([summary](https://blog.athina.ai/everything-of-thoughts-defying-the-law-of-penrose-triangle-for-thought-generation) | [paper](https://arxiv.org/abs/2311.04254))
- Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models ([summary](https://blog.athina.ai/algorithm-of-thoughts-enhancing-exploration-of-ideas-in-large-language-models) | [paper](https://arxiv.org/abs/2308.10379))
- Demystifying Chains, Trees, and Graphs of Thoughts ([summary](https://blog.athina.ai/demystifying-chains-trees-and-graphs-of-thoughts) | [paper](https://arxiv.org/abs/2401.14295))
- Large Language Models are reasoners with Self-Verification ([summary](https://blog.athina.ai/large-language-models-are-reasoners-with-self-verification) | [paper](https://arxiv.org/abs/2212.09561v1))
- Constitutional AI: Harmlessness from AI Feedback ([summary](https://blog.athina.ai/constitutional-ai-harmlessness-from-ai-feedback) | [paper](https://arxiv.org/abs/2212.08073))
- Multimodal Chain-of-Thought Reasoning in Language Models ([summary](https://blog.athina.ai/multimodal-chain-of-thought-reasoning-in-language-models) | [paper](https://arxiv.org/abs/2302.00923))
- The Capacity for Moral Self-Correction in Large Language Models ([summary](https://blog.athina.ai/the-capacity-for-moral-self-correction-in-large-language-models) | [paper](https://arxiv.org/abs/2302.07459))
- ART: Automatic multi-step reasoning and tool-use for large language models ([summary](https://blog.athina.ai/art-automatic-multi-step-reasoning-and-tool-use-for-large-language-models) | [paper](https://arxiv.org/abs/2303.09014))
- REFINER: Reasoning Feedback on Intermediate Representations ([summary](https://blog.athina.ai/refiner-reasoning-feedback-on-intermediate-representations) | [paper](https://arxiv.org/abs/2304.01904))
- Why think step by step? Reasoning emerges from the locality of experience ([summary](https://blog.athina.ai/why-think-step-by-step-reasoning-emerges-from-the-locality-of-experience) | [paper](https://arxiv.org/abs/2304.03843))
- SatLM: Satisfiability-Aided Language Models Using Declarative Prompting ([summary](https://blog.athina.ai/satlm-satisfiability-aided-language-models-using-declarative-prompting) | [paper](https://arxiv.org/abs/2305.09656))
- Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling ([summary](https://blog.athina.ai/reprompting-automated-chain-of-thought-prompt-inference-through-gibbs-sampling) | [paper](https://arxiv.org/abs/2305.09993))
- What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning ([summary](https://blog.athina.ai/what-in-context-learning-learns-in-context-disentangling-task-recognition-and-task-learning) | [paper](https://arxiv.org/abs/2305.09731))
- TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding ([summary](https://blog.athina.ai/treeprompt-learning-to-compose-tree-prompts-for-explainable-visual-grounding) | [paper](https://arxiv.org/abs/2305.11497))
- Meta-in-context learning in large language models ([summary](https://blog.athina.ai/meta-in-context-learning-in-large-language-models) | [paper](https://arxiv.org/abs/2305.12907))
- Explaining Emergent In-Context Learning as Kernel Regression ([summary](https://blog.athina.ai/explaining-emergent-in-context-learning-as-kernel-regression) | [paper](https://arxiv.org/abs/2305.12766))
- Can We Edit Factual Knowledge by In-Context Learning? ([summary](https://blog.athina.ai/can-we-edit-factual-knowledge-by-in-context-learning) | [paper](https://arxiv.org/abs/2305.12740))
- Reasoning with Language Model is Planning with World Model ([summary](https://blog.athina.ai/reasoning-with-language-model-is-planning-with-world-model) | [paper](https://arxiv.org/abs/2305.14992v1))
- MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting ([summary](https://blog.athina.ai/multitool-cot-gpt-3-can-use-multiple-external-tools-with-chain-of-thought-prompting) | [paper](https://arxiv.org/abs/2305.16896))
- Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses ([summary](https://blog.athina.ai/less-likely-brainstorming-using-language-models-to-generate-alternative-hypotheses) | [paper](https://arxiv.org/abs/2305.19339))
- Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading ([summary](https://blog.athina.ai/walking-down-the-memory-maze-beyond-context-limit-through-interactive-reading) | [paper](https://arxiv.org/abs/2310.05029))
- Emergent Abilities of Large Language Models ([summary](https://blog.athina.ai/emergent-abilities-of-large-language-models) | [paper](https://arxiv.org/abs/2206.07682))
- Reasoning with Language Model Prompting: A Survey ([summary](https://blog.athina.ai/reasoning-with-language-model-prompting-a-survey) | [paper](https://arxiv.org/abs/2212.09597))
- Towards Reasoning in Large Language Models: A Survey ([summary](https://blog.athina.ai/towards-reasoning-in-large-language-models-a-survey) | [paper](https://arxiv.org/abs/2212.10403))
- Augmented Language Models: a Survey ([summary](https://blog.athina.ai/augmented-language-models-a-survey) | [paper](https://arxiv.org/abs/2302.07842))
- Natural Language Reasoning, A Survey ([summary](https://blog.athina.ai/natural-language-reasoning-a-survey) | [paper](https://arxiv.org/abs/2303.14725))
- Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond ([summary](https://blog.athina.ai/reinforcement-learning-in-the-era-of-llms-what-is-essential-what-is-needed-an-rl-perspective-on-rlhf-prompting-and-beyond) | [paper](https://arxiv.org/abs/2310.06147))
- Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language Models ([summary](https://blog.athina.ai/post-semantic-thinking-a-robust-strategy-to-distill-reasoning-capacity-from-large-language-models) | [paper](https://arxiv.org/html/2404.09170v1))
