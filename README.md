# ai-research-papers
Summaries of AI Research Papers

## Prompt Engineering
- Controlling Personality Style in Dialogue with Zero-Shot Prompt-Based Learning ([summary](https://blog.athina.ai/controlling-personality-style-in-dialogue-with-zero-shot-prompt-based-learning) | [paper](https://arxiv.org/abs/2302.03848))
- Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models ([summary](https://blog.athina.ai/soft-prompt-tuning-for-augmenting-dense-retrieval-with-large-language-models) | [paper](https://arxiv.org/abs/2307.08303))
- Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration ([summary](https://blog.athina.ai/multimodal-prompt-perceiver-empower-adaptiveness-generalizability-and-fidelity-for-all-in-one-image-restoration) | [paper](https://arxiv.org/abs/2312.02918))
- BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information Retrieval ([summary](https://blog.athina.ai/bim-gpt-a-prompt-based-virtual-assistant-framework-for-bim-information-retrieval) | [paper](https://arxiv.org/abs/2304.09333))
- Prompt-In-Prompt Learning for Universal Image Restoration ([summary](https://blog.athina.ai/prompt-in-prompt-learning-for-universal-image-restoration) | [paper](https://arxiv.org/abs/2312.05038))
- AutoHint: Automatic Prompt Optimization with Hint Generation ([summary](https://blog.athina.ai/autohint-automatic-prompt-optimization-with-hint-generation) | [paper](https://arxiv.org/abs/2307.07415))
- Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations ([summary](https://blog.athina.ai/prompt-tuning-large-language-models-on-personalized-aspect-extraction-for-recommendations) | [paper](https://arxiv.org/abs/2306.01475))
- Prompt Middleware: Mapping Prompts for Large Language Models to UI Affordances ([summary](https://blog.athina.ai/prompt-middleware-mapping-prompts-for-large-language-models-to-ui-affordances) | [paper](https://arxiv.org/abs/2307.01142))
- Prompt-Guided Transformers for End-to-End Open-Vocabulary Object Detection ([summary](https://blog.athina.ai/prompt-guided-transformers-for-end-to-end-open-vocabulary-object-detection) | [paper](https://arxiv.org/abs/2303.14386))
- Prompt-based Node Feature Extractor for Few-shot Learning on Text-Attributed Graphs ([summary](https://blog.athina.ai/prompt-based-node-feature-extractor-for-few-shot-learning-on-text-attributed-graphs) | [paper](https://arxiv.org/abs/2309.02848))
- You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content ([summary](https://blog.athina.ai/you-only-prompt-once-on-the-capabilities-of-prompt-learning-on-large-language-models-to-tackle-toxic-content) | [paper](https://arxiv.org/abs/2308.05596))
- PBNR: Prompt-based News Recommender System ([summary](https://blog.athina.ai/pbnr-prompt-based-news-recommender-system) | [paper](https://arxiv.org/abs/2304.07862))
- Generalized Graph Prompt: Toward a Unification of Pre-Training and Downstream Tasks on Graphs ([summary](https://blog.athina.ai/generalized-graph-prompt-toward-a-unification-of-pre-training-and-downstream-tasks-on-graphs) | [paper](https://arxiv.org/abs/2311.15317))
- HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models ([summary](https://blog.athina.ai/hd-painter-high-resolution-and-prompt-faithful-text-guided-image-inpainting-with-diffusion-models) | [paper](https://arxiv.org/abs/2312.14091))
- PromptTTS 2: Describing and Generating Voices with Text Prompt ([summary](https://blog.athina.ai/prompttts-2-describing-and-generating-voices-with-text-prompt) | [paper](https://arxiv.org/abs/2309.02285))
- Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies ([summary](https://blog.athina.ai/enhancing-few-shot-text-to-sql-capabilities-of-large-language-models-a-study-on-prompt-design-strategies) | [paper](https://arxiv.org/abs/2305.12586))
- Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models ([summary](https://blog.athina.ai/negative-prompt-inversion-fast-image-inversion-for-editing-with-text-guided-diffusion-models) | [paper](https://arxiv.org/abs/2305.16807))
- LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression ([summary](https://blog.athina.ai/longllmlingua-accelerating-and-enhancing-llms-in-long-context-scenarios-via-prompt-compression) | [paper](https://arxiv.org/abs/2310.06839))
- Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers ([summary](https://blog.athina.ai/connecting-large-language-models-with-evolutionary-algorithms-yields-powerful-prompt-optimizers) | [paper](https://arxiv.org/abs/2309.08532))
- Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting ([summary](https://blog.athina.ai/quantifying-language-models-sensitivity-to-spurious-features-in-prompt-design-or-how-i-learned-to-start-worrying-about-prompt-formatting) | [paper](https://arxiv.org/abs/2310.11324))
- ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design ([summary](https://blog.athina.ai/chatgpt-prompt-patterns-for-improving-code-quality-refactoring-requirements-elicitation-and-software-design) | [paper](https://arxiv.org/abs/2303.07839))
- Reflexion: Language Agents with Verbal Reinforcement Learning ([summary](https://blog.athina.ai/reflexion-language-agents-with-verbal-reinforcement-learning) | [paper](https://arxiv.org/pdf/2303.11366.pdf))
- A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning Program Repair ([summary](https://blog.athina.ai/a-study-on-prompt-design-advantages-and-limitations-of-chatgpt-for-deep-learning-program-repair) | [paper](https://arxiv.org/abs/2304.0819))
- Prompt-Free Diffusion: Taking 'Text' out of Text-to-Image Diffusion Models ([summary](https://blog.athina.ai/prompt-free-diffusion-taking-text-out-of-text-to-image-diffusion-models) | [paper](https://arxiv.org/abs/2305.16223))
- How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings ([summary](https://blog.athina.ai/how-to-prompt-llms-for-text-to-sql-a-study-in-zero-shot-single-domain-and-cross-domain-settings) | [paper](https://arxiv.org/abs/2305.11853))
- Exploring EFL students' prompt engineering in human-AI story writing: an Activity Theory perspective ([summary](https://blog.athina.ai/exploring-efl-students-prompt-engineering-in-human-ai-story-writing-an-activity-theory-perspective) | [paper](https://arxiv.org/abs/2306.01798))
- A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications ([summary](https://blog.athina.ai/a-systematic-survey-of-prompt-engineering-in-large-language-models-techniques-and-applications) | [paper](https://arxiv.org/abs/2402.07927))
- Wordflow: Social Prompt Engineering for Large Language Models ([summary](https://blog.athina.ai/wordflow-social-prompt-engineering-for-large-language-models) | [paper](https://arxiv.org/abs/2401.14447))
- Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation ([summary](https://blog.athina.ai/automated-black-box-prompt-engineering-for-personalized-text-to-image-generation) | [paper](https://arxiv.org/abs/2403.1910))
- Exploring Prompt Engineering Practices in the Enterprise ([summary](https://blog.athina.ai/exploring-prompt-engineering-practices-in-the-enterprise) | [paper](https://arxiv.org/abs/2403.0895))
- ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level Generation ([summary](https://blog.athina.ai/chatgpt4pcg-2-competition-prompt-engineering-for-science-birds-level-generation) | [paper](https://arxiv.org/abs/2403.02610))
- MedPromptExtract (Medical Data Extraction Tool): Anonymization and Hi-fidelity Automated data extraction using NLP and prompt engineering ([summary](https://blog.athina.ai/medpromptextract-medical-data-extraction-tool-anonymization-and-hi-fidelity-automated-data-extraction-using-nlp-and-prompt-engineering) | [paper](https://arxiv.org/abs/2405.02664))
- Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering ([summary](https://blog.athina.ai/exploring-the-intersection-of-large-language-models-and-agent-based-modeling-via-prompt-engineering) | [paper](https://arxiv.org/abs/2308.07411))
- Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation ([summary](https://blog.athina.ai/large-language-models-and-prompt-engineering-for-biomedical-query-focused-multi-document-summarisation) | [paper](https://arxiv.org/abs/2312.04344))
- Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies ([summary](https://blog.athina.ai/enhancing-medical-task-performance-in-gpt-4v-a-comprehensive-study-on-prompt-engineering-strategies) | [paper](https://arxiv.org/abs/2312.04344))
- Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT ([summary](https://blog.athina.ai/cases-of-efl-secondary-students-prompt-engineering-pathways-to-complete-a-writing-task-with-chatgpt) | [paper](https://arxiv.org/abs/2307.05493))
- Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4 ([summary](https://blog.athina.ai/prompt-engineering-assisted-malware-dynamic-analysis-using-gpt-4) | [paper](https://arxiv.org/abs/2312.08317))
- Prompt-Engineering and Transformer-based Question Generation and Evaluation ([summary](https://blog.athina.ai/prompt-engineering-and-transformer-based-question-generation-and-evaluation) | [paper](https://arxiv.org/abs/2310.18867))
- PEACE: Prompt Engineering Automation for CLIPSeg Enhancement in Aerial Robotics ([summary](https://blog.athina.ai/peace-prompt-engineering-automation-for-clipseg-enhancement-in-aerial-robotics) | [paper](https://arxiv.org/abs/2310.000))
- To be or not to be? an exploration of continuously controllable prompt engineering ([summary](https://blog.athina.ai/to-be-or-not-to-be-an-exploration-of-continuously-controllable-prompt-engineering) | [paper](https://arxiv.org/abs/2311.09773))
- A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models ([summary](https://blog.athina.ai/a-systematic-survey-of-prompt-engineering-on-vision-language-foundation-models) | [paper](https://arxiv.org/abs/2307.12980))
- Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering ([summary](https://blog.athina.ai/prompting-ai-art-an-investigation-into-the-creative-skill-of-prompt-engineering) | [paper](https://arxiv.org/abs/2303.13534))
- Prompt Engineering a Prompt Engineer ([summary](https://blog.athina.ai/prompt-engineering-a-prompt-engineer) | [paper](https://arxiv.org/abs/2311.05661))
- Prompt Engineering for Healthcare: Methodologies and Applications ([summary](https://blog.athina.ai/prompt-engineering-for-healthcare-methodologies-and-applications) | [paper](https://arxiv.org/abs/2304.14670))
- Large Language Models Are Human-Level Prompt Engineers ([summary](https://blog.athina.ai/large-language-models-are-human-level-prompt-engineers) | [paper](https://arxiv.org/abs/2211.01910))
- DocPrompting: Generating Code by Retrieving the Docs ([summary](https://blog.athina.ai/docprompting-generating-code-by-retrieving-the-docs) | [paper](https://arxiv.org/abs/2207.05987))
- PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization ([summary](https://blog.athina.ai/promptagent-strategic-planning-with-language-models-enables-expert-level-prompt-optimization) | [paper](https://arxiv.org/abs/2310.16427))
- LLM Guided Evolution -- The Automation of Models Advancing Models ([summary](https://blog.athina.ai/llm-guided-evolution----the-automation-of-models-advancing-models) | [paper](https://arxiv.org/abs/2403.11446))
- Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review ([summary](https://blog.athina.ai/unleashing-the-potential-of-prompt-engineering-in-large-language-models-a-comprehensive-review) | [paper](https://arxiv.org/pdf/2310.14735))
- Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery ([summary](https://blog.athina.ai/hard-prompts-made-easy-gradient-based-discrete-optimization-for-prompt-tuning-and-discovery) | [paper](https://arxiv.org/abs/2302.03668))
- Compositional Exemplars for In-context Learning ([summary](https://blog.athina.ai/compositional-exemplars-for-in-context-learning) | [paper](https://arxiv.org/abs/2302.05698))
- SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains ([summary](https://blog.athina.ai/switchprompt-learning-domain-specific-gated-soft-prompts-for-classification-in-low-resource-domains) | [paper](https://arxiv.org/abs/2302.06868))
- GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks ([summary](https://blog.athina.ai/graphprompt-unifying-pre-training-and-downstream-tasks-for-graph-neural-networks) | [paper](https://arxiv.org/abs/2302.08043))
- Scalable Prompt Generation for Semi-supervised Learning with Language Models ([summary](https://blog.athina.ai/scalable-prompt-generation-for-semi-supervised-learning-with-language-models) | [paper](https://arxiv.org/abs/2302.09236))
- Guiding Large Language Models via Directional Stimulus Prompting ([summary](https://blog.athina.ai/guiding-large-language-models-via-directional-stimulus-prompting) | [paper](https://arxiv.org/abs/2302.11520))
- A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT ([summary](https://blog.athina.ai/a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgpt) | [paper](https://arxiv.org/abs/2302.11382))
- EvoPrompting: Language Models for Code-Level Neural Architecture Search ([summary](https://blog.athina.ai/evoprompting-language-models-for-code-level-neural-architecture-search) | [paper](https://arxiv.org/abs/2302.14838))
- Dynamic Prompting: A Unified Framework for Prompt Tuning ([summary](https://blog.athina.ai/dynamic-prompting-a-unified-framework-for-prompt-tuning) | [paper](https://arxiv.org/abs/2303.02909))
- CAMEL: Communicative Agents for 'Mind' Exploration of Large Language Model Society ([summary](https://blog.athina.ai/camel-communicative-agents-for-mind-exploration-of-large-language-model-society) | [paper](https://arxiv.org/abs/2303.17760))
- Boosted Prompt Ensembles for Large Language Models ([summary](https://blog.athina.ai/boosted-prompt-ensembles-for-large-language-models) | [paper](https://arxiv.org/abs/2304.05970))
- Efficient Prompting via Dynamic In-Context Learning ([summary](https://blog.athina.ai/efficient-prompting-via-dynamic-in-context-learning) | [paper](https://arxiv.org/abs/2305.11170))
- Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt ([summary](https://blog.athina.ai/compress-then-prompt-improving-accuracy-efficiency-trade-off-of-llm-inference-with-transferable-prompt) | [paper](https://arxiv.org/abs/2305.11186))
- Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models ([summary](https://blog.athina.ai/plan-and-solve-prompting-improving-zero-shot-chain-of-thought-reasoning-by-large-language-models) | [paper](https://arxiv.org/abs/2305.04091v3))
- Hierarchical Prompting Assists Large Language Model on Web Navigation ([summary](https://blog.athina.ai/hierarchical-prompting-assists-large-language-model-on-web-navigation) | [paper](https://arxiv.org/abs/2305.14257))
- Better Zero-Shot Reasoning with Self-Adaptive Prompting ([summary](https://blog.athina.ai/better-zero-shot-reasoning-with-self-adaptive-prompting) | [paper](https://arxiv.org/abs/2305.14106))
- PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents ([summary](https://blog.athina.ai/pearl-prompting-large-language-models-to-plan-and-execute-actions-over-long-documents) | [paper](https://arxiv.org/abs/2305.14564v1))
- Exploring Lottery Prompts for Pre-trained Language Models ([summary](https://blog.athina.ai/exploring-lottery-prompts-for-pre-trained-language-models) | [paper](https://arxiv.org/abs/2305.19500))
- Graph of Thoughts: Solving Elaborate Problems with Large Language Models ([summary](https://blog.athina.ai/graph-of-thoughts-solving-elaborate-problems-with-large-language-models) | [paper](https://arxiv.org/abs/2308.09687v2))
- From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting ([summary](https://blog.athina.ai/from-sparse-to-dense-gpt-4-summarization-with-chain-of-density-prompting) | [paper](https://arxiv.org/abs/2309.04269))
- Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing ([summary](https://blog.athina.ai/pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processing) | [paper](https://arxiv.org/abs/2107.13586))
- A Taxonomy of Prompt Modifiers for Text-To-Image Generation ([summary](https://blog.athina.ai/a-taxonomy-of-prompt-modifiers-for-text-to-image-generation) | [paper](https://arxiv.org/abs/2204.13988))
- Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study ([summary](https://blog.athina.ai/jailbreaking-chatgpt-via-prompt-engineering-an-empirical-study) | [paper](https://arxiv.org/abs/2305.13860))
- Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data ([summary](https://blog.athina.ai/automatic-prompt-augmentation-and-selection-with-chain-of-thought-from-labeled-data) | [paper](https://arxiv.org/abs/2302.12822))
- Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following ([summary](https://blog.athina.ai/investigating-the-effectiveness-of-task-agnostic-prefix-prompt-for-instruction-following) | [paper](https://arxiv.org/abs/2302.14691))
- Model-tuning Via Prompts Makes NLP Models Adversarially Robust ([summary](https://blog.athina.ai/model-tuning-via-prompts-makes-nlp-models-adversarially-robust) | [paper](https://arxiv.org/abs/2303.07320))
- UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation ([summary](https://blog.athina.ai/uprise-universal-prompt-retrieval-for-improving-zero-shot-evaluation) | [paper](https://arxiv.org/abs/2303.08518))
- A Comprehensive Survey on Instruction Following ([summary](https://blog.athina.ai/uprise-universal-prompt-retrieval-for-improving-zero-shot-evaluation) | [paper](https://arxiv.org/abs/2303.10475))
- Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models ([summary](https://blog.athina.ai/knowledge-card-filling-llms-knowledge-gaps-with-plug-in-specialized-language-models) | [paper](https://arxiv.org/abs/2305.09955))
- Tree of Thoughts: Deliberate Problem Solving with Large Language Models ([summary](https://blog.athina.ai/tree-of-thoughts-deliberate-problem-solving-with-large-language-models) | [paper](https://arxiv.org/abs/2305.10601))
- Post Hoc Explanations of Language Models Can Improve Language Models ([summary](https://blog.athina.ai/post-hoc-explanations-of-language-models-can-improve-language-models) | [paper](https://arxiv.org/abs/2305.11426))
- Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting ([summary](https://blog.athina.ai/enhancing-large-language-models-against-inductive-instructions-with-dual-critique-prompting) | [paper](https://arxiv.org/abs/2305.13733))
- Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation ([summary](https://blog.athina.ai/skeleton-of-thought-prompting-llms-for-efficient-parallel-generation) | [paper](https://arxiv.org/abs/2307.15337))
- Re-Reading Improves Reasoning in Large Language Models ([summary](https://blog.athina.ai/re-reading-improves-reasoning-in-large-language-models) | [paper](https://arxiv.org/abs/2309.06275))
- Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers ([summary](https://blog.athina.ai/connecting-large-language-models-with-evolutionary-algorithms-yields-powerful-prompt-optimizers) | [paper](https://arxiv.org/abs/2309.08532))
- LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models ([summary](https://blog.athina.ai/llmlingua-compressing-prompts-for-accelerated-inference-of-large-language-models) | [paper](https://arxiv.org/abs/2310.05736))
- Large Language Models as Analogical Reasoners ([summary](https://blog.athina.ai/large-language-models-as-analogical-reasoners) | [paper](https://arxiv.org/abs/2310.01714))
- Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4 ([summary](https://blog.athina.ai/principled-instructions-are-all-you-need-for-questioning-llama-1/2-gpt-3.5/4) | [paper](https://arxiv.org/abs/2312.16171v1))
- Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic ([summary](https://blog.athina.ai/enhancing-zero-shot-chain-of-thought-reasoning-in-large-language-models-through-logic) | [paper](https://arxiv.org/abs/2309.13339))
- Prompt Design and Engineering: Introduction and Advanced Methods ([summary](https://blog.athina.ai/prompt-design-and-engineering-introduction-and-advanced-methods) | [paper](https://arxiv.org/abs/2401.14423))
- Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions ([summary](https://blog.athina.ai/probabilistic-tree-of-thought-reasoning-for-answering-knowledge-intensive-complex-questions) | [paper](https://arxiv.org/abs/2311.13982))
- Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks ([summary](https://blog.athina.ai/search-in-the-chain-interactively-enhancing-large-language-models-with-search-for-knowledge-intensive-tasks) | [paper](https://arxiv.org/abs/2304.14732))
- Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources ([summary](https://blog.athina.ai/chain-of-knowledge-grounding-large-language-models-via-dynamic-knowledge-adapting-over-heterogeneous-sources) | [paper](https://arxiv.org/abs/2305.13269))
- Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning ([summary](https://blog.athina.ai/semi-structured-chain-of-thought-integrating-multiple-sources-of-knowledge-for-improved-language-model-reasoning) | [paper](https://arxiv.org/abs/2311.08505))
- EntGPT: Linking Generative Large Language Models with Knowledge Bases ([summary](https://blog.athina.ai/entgpt-linking-generative-large-language-models-with-knowledge-bases) | [paper](https://arxiv.org/abs/2402.06738))
- Chain-of-Verification Reduces Hallucination in Large Language Models ([summary](https://blog.athina.ai/chain-of-verification-reduces-hallucination-in-large-language-models) | [paper](https://arxiv.org/abs/2309.11495))
- Reflexion: Language Agents with Verbal Reinforcement Learning ([summary](https://blog.athina.ai/reflexion-language-agents-with-verbal-reinforcement-learning) | [paper](https://arxiv.org/pdf/2303.11366.pdf))


## Hallucinations
- Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness [summary](https://blog.athina.ai/exploring-the-relationship-between-llm-hallucinations-and-prompt-linguistic-nuances-readability-formality-and-concreteness) | [paper](https://arxiv.org/abs/2309.11064)
- Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond ([summary](https://blog.athina.ai/re-imagine-the-negative-prompt-algorithm-transform-2d-diffusion-into-3d-alleviate-janus-problem-and-beyond) | [paper](https://arxiv.org/abs/2304.04968))
- AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection ([summary](https://blog.athina.ai/anomalyclip-object-agnostic-prompt-learning-for-zero-shot-anomaly-detection) | [paper](https://arxiv.org/abs/2310.18961))
- DiffusionGPT: LLM-Driven Text-to-Image Generation System ([summary](https://blog.athina.ai/diffusiongpt-llm-driven-text-to-image-generation-system) | [paper](https://arxiv.org/abs/2401.10061))
- Chain-of-Verification Reduces Hallucination in Large Language Models ([summary](https://blog.athina.ai/chain-of-verification-reduces-hallucination-in-large-language-models) | [paper](https://arxiv.org/abs/2309.11495))
- A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions ([summary](https://blog.athina.ai/a-survey-on-hallucination-in-large-language-models-principles-taxonomy-challenges-and-open-questions) | [paper](https://arxiv.org/abs/2311.052))
- AutoHall: Automated Hallucination Dataset Generation for Large Language Models ([summary](https://blog.athina.ai/autohall-automated-hallucination-dataset-generation-for-large-language-models) | [paper](https://arxiv.org/abs/2310.0025))
- Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models ([summary](https://blog.athina.ai/siren-s-song-in-the-ai-ocean-a-survey-on-hallucination-in-large-language-models) | [paper](https://arxiv.org/abs/2309.01219))
- Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation ([summary](https://blog.athina.ai/self-contradictory-hallucinations-of-large-language-models-evaluation-detection-and-mitigation) | [paper](https://arxiv.org/abs/2305.15852))
- Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination ([summary](https://blog.athina.ai/deficiency-of-large-language-models-in-finance-an-empirical-examination-of-hallucination) | [paper](https://arxiv.org/abs/2311.15548))
- Factuality of Large Language Models in the Year 2024 ([summary](https://blog.athina.ai/factuality-of-large-language-models-in-the-year-2024) | [paper](https://arxiv.org/abs/2402.02420))
- Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification ([summary](https://blog.athina.ai/ever-mitigating-hallucination-in-large-language-models-through-real-time-verification-and-rectification) | [paper](https://arxiv.org/html/2311.09114v2))

## RAG
- Boosted Prompt Ensembles for Large Language Models ([summary](https://blog.athina.ai/boosted-prompt-ensembles-for-large-language-models) | [paper](https://arxiv.org/abs/2304.05970))
- Chit-Chat or Deep Talk: Prompt Engineering for Process Mining ([summary](https://blog.athina.ai/chit-chat-or-deep-talk-prompt-engineering-for-process-mining) | [paper](https://arxiv.org/abs/2307.09909))
- Prompt Engineering for Transformer-based Chemical Similarity Search Identifies Structurally Distinct Functional Analogues ([summary](https://blog.athina.ai/prompt-engineering-for-transformer-based-chemical-similarity-search-identifies-structurally-distinct-functional-analogues) | [paper](https://arxiv.org/abs/2305.16330))
- Recitation-Augmented Language Models ([summary](https://blog.athina.ai/recitation-augmented-language-models) | [paper](https://arxiv.org/abs/2210.01296))
- RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval ([summary](https://blog.athina.ai/rnns-are-not-transformers-yet-the-key-bottleneck-on-in-context-retrieval) | [paper](https://arxiv.org/abs/2402.18510))
- Retrieval-Augmented Thought Process as Sequential Decision Making ([summary](https://blog.athina.ai/retrieval-augmented-thought-process-as-sequential-decision-making) | [paper](https://arxiv.org/abs/2402.07812))
- Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP ([summary](https://blog.athina.ai/demonstrate-search-predict-composing-retrieval-and-language-models-for-knowledge-intensive-nlp) | [paper](https://arxiv.org/abs/2212.14024))
- Language Is Not All You Need: Aligning Perception with Language Models ([summary](https://blog.athina.ai/language-is-not-all-you-need-aligning-perception-with-language-models) | [paper](https://arxiv.org/abs/2302.14045))
- Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer ([summary](https://blog.athina.ai/structure-pretraining-and-prompt-tuning-for-knowledge-graph-transfer) | [paper](https://arxiv.org/abs/2303.03922))
- Pre-Training to Learn in Context ([summary](https://blog.athina.ai/pre-training-to-learn-in-context) | [paper](https://arxiv.org/abs/2305.09137))
- The Web Can Be Your Oyster for Improving Large Language Models ([summary](https://blog.athina.ai/the-web-can-be-your-oyster-for-improving-large-language-models) | [paper](https://arxiv.org/abs/2305.10998))
- A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models ([summary](https://blog.athina.ai/a-step-closer-to-comprehensive-answers-constrained-multi-stage-question-decomposition-with-large-language-models) | [paper](https://arxiv.org/abs/2311.07491))
- Active Retrieval Augmented Generation ([summary](https://blog.athina.ai/active-retrieval-augmented-generation) | [paper](https://arxiv.org/abs/2305.06983))
- KnowGPT: Knowledge Injection for Large Language Models ([summary](https://blog.athina.ai/knowgpt-knowledge-injection-for-large-language-models) | [paper](https://arxiv.org/abs/2312.06185))

## Tutorial 
- SPROUT: Authoring Programming Tutorials with Interactive Visualization of Large Language Model Generation Process ([summary](https://blog.athina.ai/sprout-authoring-programming-tutorials-with-interactive-visualization-of-large-language-model-generation-process) | [paper](https://arxiv.org/abs/2312.01801))

## Dataset Generation
- Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition ([summary](https://blog.athina.ai/ignore-this-title-and-hackaprompt-exposing-systemic-vulnerabilities-of-llms-through-a-global-scale-prompt-hacking-competition) | [paper](https://arxiv.org/abs/2311.16119))
- Language Prompt for Autonomous Driving ([summary](https://blog.athina.ai/language-prompt-for-autonomous-driving) | [paper](https://arxiv.org/abs/2309.04379))
- Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training ([summary](https://blog.athina.ai/towards-large-scale-3d-representation-learning-with-multi-dataset-point-prompt-training) | [paper](https://arxiv.org/abs/2308.09718))
- Text2MDT: Extracting Medical Decision Trees from Medical Texts ([summary](https://blog.athina.ai/text2mdt-extracting-medical-decision-trees-from-medical-texts) | [paper](https://arxiv.org/abs/2401.02034))
- Analyzing Toxicity in Deep Conversations: A Reddit Case Study ([summary](https://blog.athina.ai/analyzing-toxicity-in-deep-conversations-a-reddit-case-study) | [paper](https://arxiv.org/abs/2404.07879))
- GuReT: Distinguishing Guilt and Regret related Text ([summary](https://blog.athina.ai/guret-distinguishing-guilt-and-regret-related-text) | [paper](https://arxiv.org/abs/2401.16541))
- Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples ([summary](https://blog.athina.ai/large-language-models-are-few-shot-generators-proposing-hybrid-prompt-algorithm-to-generate-webshell-escape-samples) | [paper](https://arxiv.org/abs/2402.07408))
- Mixture of Soft Prompts for Controllable Data Generation ([summary](https://blog.athina.ai/mixture-of-soft-prompts-for-controllable-data-generation) | [paper](https://arxiv.org/abs/2303.01580))
- WizardLM: Empowering Large Language Models to Follow Complex Instructions ([summary](https://blog.athina.ai/wizardlm-empowering-large-language-models-to-follow-complex-instructions) | [paper](https://arxiv.org/abs/2304.12244))

## Reasoning
- An automatically discovered chain-of-thought prompt generalizes to novel models and datasets ([summary](https://blog.athina.ai/an-automatically-discovered-chain-of-thought-prompt-generalizes-to-novel-models-and-datasets) | [paper](https://arxiv.org/abs/2305.02897))
- Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL ([summary](https://blog.athina.ai/query-dependent-prompt-evaluation-and-optimization-with-offline-inverse-rl) | [paper](https://arxiv.org/abs/2309.06553))
- Divide and Prompt: Chain of Thought Prompting for Text-to-SQL ([summary](https://blog.athina.ai/divide-and-prompt-chain-of-thought-prompting-for-text-to-sql) | [paper](https://arxiv.org/abs/2304.11556))
- Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling ([summary](https://blog.athina.ai/reprompting-automated-chain-of-thought-prompt-inference-through-gibbs-sampling) | [paper](https://arxiv.org/abs/2305.09993))
- LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models ([summary](https://blog.athina.ai/llm-grounded-diffusion-enhancing-prompt-understanding-of-text-to-image-diffusion-models-with-large-language-models) | [paper](https://arxiv.org/abs/2305.13655))
- Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution ([summary](https://blog.athina.ai/promptbreeder-self-referential-self-improvement-via-prompt-evolution) | [paper](https://arxiv.org/abs/2309.16797))
- Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT ([summary](https://blog.athina.ai/graph-toolformer-to-empower-llms-with-graph-reasoning-ability-via-prompt-augmented-by-chatgpt) | [paper](https://arxiv.org/abs/2304.11116))
- Understanding prompt engineering may not require rethinking generalization ([summary](https://blog.athina.ai/understanding-prompt-engineering-may-not-require-rethinking-generalization) | [paper](https://arxiv.org/abs/2310.03957))
- Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought ([summary](https://blog.athina.ai/language-models-are-greedy-reasoners-a-systematic-formal-analysis-of-chain-of-thought) | [paper](https://arxiv.org/abs/2210.01240v3))
- Decomposed Prompting: A Modular Approach for Solving Complex Tasks ([summary](https://blog.athina.ai/decomposed-prompting-a-modular-approach-for-solving-complex-tasks) | [paper](https://arxiv.org/abs/2210.02406))
- ReAct: Synergizing Reasoning and Acting in Language Models ([summary](https://blog.athina.ai/react-synergizing-reasoning-and-acting-in-language-models) | [paper](https://arxiv.org/abs/2210.03629))
- PAL: Program-aided Language Models ([summary](https://blog.athina.ai/pal-program-aided-language-models) | [paper](https://arxiv.org/abs/2211.10435))
- Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning ([summary](https://blog.athina.ai/dynamic-prompt-learning-via-policy-gradient-for-semi-structured-mathematical-reasoning) | [paper](https://arxiv.org/abs/2209.14610))
- Making Large Language Models Better Reasoners with Step-Aware Verifier ([summary](https://blog.athina.ai/making-large-language-models-better-reasoners-with-step-aware-verifier) | [paper](https://arxiv.org/abs/2206.02336))
- Large Language Models are Zero-Shot Reasoners ([summary](https://blog.athina.ai/large-language-models-are-zero-shot-reasoners) | [paper](https://arxiv.org/abs/2205.11916))
- Self-Consistency Improves Chain of Thought Reasoning in Language Models ([summary](https://blog.athina.ai/self-consistency-improves-chain-of-thought-reasoning-in-language-models) | [paper](https://arxiv.org/abs/2203.11171))
- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models ([summary](https://blog.athina.ai/chain-of-thought-prompting-elicits-reasoning-in-large-language-models) | [paper](https://arxiv.org/abs/2201.11903))
- LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding ([summary](https://blog.athina.ai/layoutllm-layout-instruction-tuning-with-large-language-models-for-document-understanding) | [paper](https://arxiv.org/abs/2404.05225))
- Inferring Properties of Graph Neural Networks ([summary](https://blog.athina.ai/inferring-properties-of-graph-neural-networks) | [paper](https://arxiv.org/abs/2401.03790))
- RoT: Enhancing Large Language Models with Reflection on Search Trees ([summary](https://blog.athina.ai/rot-enhancing-large-language-models-with-reflection-on-search-trees) | [paper](https://arxiv.org/abs/2404.05449))
- STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent ([summary](https://blog.athina.ai/stamp-differentiable-task-and-motion-planning-via-stein-variational-gradient-descent) | [paper](https://arxiv.org/abs/2310.01775))
- Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting ([summary](https://blog.athina.ai/temporal-data-meets-llm----explainable-financial-time-series-forecasting) | [paper](https://arxiv.org/abs/2306.11025))
- GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations ([summary](https://blog.athina.ai/gtbench-uncovering-the-strategic-reasoning-limitations-of-llms-via-game-theoretic-evaluations) | [paper](https://arxiv.org/abs/2402.12348))
- On the Empirical Complexity of Reasoning and Planning in LLMs ([summary](https://blog.athina.ai/on-the-empirical-complexity-of-reasoning-and-planning-in-llms) | [paper](https://arxiv.org/abs/2404.11041))
- Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning ([summary](https://blog.athina.ai/evidence-to-generate-e2g-a-single-agent-two-step-prompting-for-context-grounded-and-retrieval-augmented-reasoning) | [paper](https://arxiv.org/abs/2401.05787))
- RAGAR, Your Falsehood RADAR: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models ([summary](https://blog.athina.ai/ragar-your-falsehood-radar-rag-augmented-reasoning-for-political-fact-checking-using-multimodal-large-language-models) | [paper](https://arxiv.org/abs/2404.12065))
- Chain-of-Thought Reasoning is a Policy Improvement Operator ([summary](https://blog.athina.ai/chain-of-thought-reasoning-is-a-policy-improvement-operator) | [paper](https://arxiv.org/abs/2309.08589))
- Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering ([summary](https://blog.athina.ai/tree-of-reviews-a-tree-based-dynamic-iterative-retrieval-framework-for-multi-hop-question-answering) | [paper](https://arxiv.org/abs/2404.14464))
- Autonomous Tree-search Ability of Large Language Models ([summary](https://blog.athina.ai/autonomous-tree-search-ability-of-large-language-models) | [paper](https://arxiv.org/abs/2310.10686))
- Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering ([summary](https://blog.athina.ai/knowledge-driven-cot-exploring-faithful-reasoning-in-llms-for-knowledge-intensive-question-answering) | [paper](https://arxiv.org/abs/2308.13259))
- Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation ([summary](https://blog.athina.ai/self-taught-optimizer-stop-recursively-self-improving-code-generation) | [paper](https://arxiv.org/abs/2310.02304))
- PathFinder: Guided Search over Multi-Step Reasoning Paths ([summary](https://blog.athina.ai/pathfinder-guided-search-over-multi-step-reasoning-paths) | [paper](https://arxiv.org/abs/2312.05180))
- Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models ([summary](https://blog.athina.ai/boosting-of-thoughts-trial-and-error-problem-solving-with-large-language-models) | [paper](https://arxiv.org/abs/2402.11140))
- MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems ([summary](https://blog.athina.ai/macm-utilizing-a-multi-agent-system-for-condition-mining-in-solving-complex-mathematical-problems) | [paper](https://arxiv.org/abs/2404.04735))
- Large Language Model Guided Tree-of-Thought ([summary](https://blog.athina.ai/large-language-model-guided-tree-of-thought) | [paper](https://arxiv.org/abs/2305.08291))
- Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning ([summary](https://blog.athina.ai/tree-of-mixed-thought-combining-fast-and-slow-thinking-for-multi-hop-visual-reasoning) | [paper](https://arxiv.org/abs/2308.09658))
- Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought ([summary](https://blog.athina.ai/boosting-logical-reasoning-in-large-language-models-through-a-new-framework-the-graph-of-thought) | [paper](https://arxiv.org/abs/2308.08614))
- Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts ([summary](https://blog.athina.ai/empowering-multi-step-reasoning-across-languages-via-tree-of-thoughts) | [paper](https://arxiv.org/abs/2311.08097))
- Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training ([summary](https://blog.athina.ai/alphazero-like-tree-search-can-guide-large-language-model-decoding-and-training) | [paper](https://arxiv.org/abs/2309.17179))
- Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation ([summary](https://blog.athina.ai/everything-of-thoughts-defying-the-law-of-penrose-triangle-for-thought-generation) | [paper](https://arxiv.org/abs/2311.04254))
- Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models ([summary](https://blog.athina.ai/algorithm-of-thoughts-enhancing-exploration-of-ideas-in-large-language-models) | [paper](https://arxiv.org/abs/2308.10379))
- Demystifying Chains, Trees, and Graphs of Thoughts ([summary](https://blog.athina.ai/demystifying-chains-trees-and-graphs-of-thoughts) | [paper](https://arxiv.org/abs/2401.14295))
- Large Language Models are reasoners with Self-Verification ([summary](https://blog.athina.ai/large-language-models-are-reasoners-with-self-verification) | [paper](https://arxiv.org/abs/2212.09561v1))
- Constitutional AI: Harmlessness from AI Feedback ([summary](https://blog.athina.ai/constitutional-ai-harmlessness-from-ai-feedback) | [paper](https://arxiv.org/abs/2212.08073))
- Multimodal Chain-of-Thought Reasoning in Language Models ([summary](https://blog.athina.ai/multimodal-chain-of-thought-reasoning-in-language-models) | [paper](https://arxiv.org/abs/2302.00923))
- The Capacity for Moral Self-Correction in Large Language Models ([summary](https://blog.athina.ai/the-capacity-for-moral-self-correction-in-large-language-models) | [paper](https://arxiv.org/abs/2302.07459))
- ART: Automatic multi-step reasoning and tool-use for large language models ([summary](https://blog.athina.ai/art-automatic-multi-step-reasoning-and-tool-use-for-large-language-models) | [paper](https://arxiv.org/abs/2303.09014))
- REFINER: Reasoning Feedback on Intermediate Representations ([summary](https://blog.athina.ai/refiner-reasoning-feedback-on-intermediate-representations) | [paper](https://arxiv.org/abs/2304.01904))
- Why think step by step? Reasoning emerges from the locality of experience ([summary](https://blog.athina.ai/why-think-step-by-step-reasoning-emerges-from-the-locality-of-experience) | [paper](https://arxiv.org/abs/2304.03843))
- SatLM: Satisfiability-Aided Language Models Using Declarative Prompting ([summary](https://blog.athina.ai/satlm-satisfiability-aided-language-models-using-declarative-prompting) | [paper](https://arxiv.org/abs/2305.09656))
- Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling ([summary](https://blog.athina.ai/reprompting-automated-chain-of-thought-prompt-inference-through-gibbs-sampling) | [paper](https://arxiv.org/abs/2305.09993))
- What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning ([summary](https://blog.athina.ai/what-in-context-learning-learns-in-context-disentangling-task-recognition-and-task-learning) | [paper](https://arxiv.org/abs/2305.09731))
- TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding ([summary](https://blog.athina.ai/treeprompt-learning-to-compose-tree-prompts-for-explainable-visual-grounding) | [paper](https://arxiv.org/abs/2305.11497))
- Meta-in-context learning in large language models ([summary](https://blog.athina.ai/meta-in-context-learning-in-large-language-models) | [paper](https://arxiv.org/abs/2305.12907))
- Explaining Emergent In-Context Learning as Kernel Regression ([summary](https://blog.athina.ai/explaining-emergent-in-context-learning-as-kernel-regression) | [paper](https://arxiv.org/abs/2305.12766))
- Can We Edit Factual Knowledge by In-Context Learning? ([summary](https://blog.athina.ai/can-we-edit-factual-knowledge-by-in-context-learning) | [paper](https://arxiv.org/abs/2305.12740))
- Reasoning with Language Model is Planning with World Model ([summary](https://blog.athina.ai/reasoning-with-language-model-is-planning-with-world-model) | [paper](https://arxiv.org/abs/2305.14992v1))
- MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting ([summary](https://blog.athina.ai/multitool-cot-gpt-3-can-use-multiple-external-tools-with-chain-of-thought-prompting) | [paper](https://arxiv.org/abs/2305.16896))
- Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses ([summary](https://blog.athina.ai/less-likely-brainstorming-using-language-models-to-generate-alternative-hypotheses) | [paper](https://arxiv.org/abs/2305.19339))
- Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading ([summary](https://blog.athina.ai/walking-down-the-memory-maze-beyond-context-limit-through-interactive-reading) | [paper](https://arxiv.org/abs/2310.05029))
- Emergent Abilities of Large Language Models ([summary](https://blog.athina.ai/emergent-abilities-of-large-language-models) | [paper](https://arxiv.org/abs/2206.07682))
- Reasoning with Language Model Prompting: A Survey ([summary](https://blog.athina.ai/reasoning-with-language-model-prompting-a-survey) | [paper](https://arxiv.org/abs/2212.09597))
- Towards Reasoning in Large Language Models: A Survey ([summary](https://blog.athina.ai/towards-reasoning-in-large-language-models-a-survey) | [paper](https://arxiv.org/abs/2212.10403))
- Augmented Language Models: a Survey ([summary](https://blog.athina.ai/augmented-language-models-a-survey) | [paper](https://arxiv.org/abs/2302.07842))
- Natural Language Reasoning, A Survey ([summary](https://blog.athina.ai/natural-language-reasoning-a-survey) | [paper](https://arxiv.org/abs/2303.14725))
- Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond ([summary](https://blog.athina.ai/reinforcement-learning-in-the-era-of-llms-what-is-essential-what-is-needed-an-rl-perspective-on-rlhf-prompting-and-beyond) | [paper](https://arxiv.org/abs/2310.06147))
- Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language Models ([summary](https://blog.athina.ai/post-semantic-thinking-a-robust-strategy-to-distill-reasoning-capacity-from-large-language-models) | [paper](https://arxiv.org/html/2404.09170v1))
  
## Foundation Model 
- TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting ([summary](https://blog.athina.ai/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting) | [paper](https://arxiv.org/abs/2310.04948))
- Prompt a Robot to Walk with Large Language Models ([summary](https://blog.athina.ai/prompt-a-robot-to-walk-with-large-language-models) | [paper](https://arxiv.org/abs/2309.09969))
- ProRes: Exploring Degradation-aware Visual Prompt for Universal Image Restoration ([summary](https://blog.athina.ai/prores-exploring-degradation-aware-visual-prompt-for-universal-image-restoration) | [paper](https://arxiv.org/abs/2306.13653))
- Segment Any Anomaly without Training via Hybrid Prompt Regularization ([summary](https://blog.athina.ai/segment-any-anomaly-without-training-via-hybrid-prompt-regularization) | [paper](https://arxiv.org/abs/2305.10724))
- SAM on Medical Images: A Comprehensive Study on Three Prompt Modes ([summary](https://blog.athina.ai/sam-on-medical-images-a-comprehensive-study-on-three-prompt-modes) | [paper](https://arxiv.org/abs/2305.00035))
- AI Chain on Large Language Model for Unsupervised Control Flow Graph Generation for Statically-Typed Partial Code ([summary](https://blog.athina.ai/ai-chain-on-large-language-model-for-unsupervised-control-flow-graph-generation-for-statically-typed-partial-code) | [paper](https://arxiv.org/abs/2306.00757))
- Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners ([summary](https://blog.athina.ai/prompt-generate-then-cache-cascade-of-foundation-models-makes-strong-few-shot-learners) | [paper](https://arxiv.org/abs/2303.02151))
- Universality and Limitations of Prompt Tuning ([summary](https://blog.athina.ai/universality-and-limitations-of-prompt-tuning) | [paper](https://arxiv.org/abs/2305.18787))
- One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era ([summary](https://blog.athina.ai/one-small-step-for-generative-ai-one-giant-leap-for-agi-a-complete-survey-on-chatgpt-in-aigc-era) | [paper](https://arxiv.org/abs/2304.06488))
- Mistral 7B: Foundation Model Research Paper Summary ([summary](https://blog.athina.ai/mistral-7b-foundation-model-research-paper-summary) | [paper](https://arxiv.org/pdf/2310.06825.pdf))

## Fine Tuning
- Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering ([summary](https://blog.athina.ai/layout-and-task-aware-instruction-prompt-for-zero-shot-document-image-question-answering) | [paper](https://arxiv.org/abs/2306.00526))
- BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP ([summary](https://blog.athina.ai/badclip-trigger-aware-prompt-learning-for-backdoor-attacks-on-clip) | [paper](https://arxiv.org/abs/2311.16194))
- TCP: Textual-based Class-aware Prompt tuning for Visual-Language Model ([summary](https://blog.athina.ai/tcp-textual-based-class-aware-prompt-tuning-for-visual-language-model) | [paper](https://arxiv.org/abs/2311.18231))
- StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing ([summary](https://blog.athina.ai/stylediffusion-prompt-embedding-inversion-for-text-based-editing) | [paper](https://arxiv.org/abs/2303.15649))
- Jatmo: Prompt Injection Defense by Task-Specific Finetuning ([summary](https://blog.athina.ai/jatmo-prompt-injection-defense-by-task-specific-finetuning) | [paper](https://arxiv.org/abs/2312.17673))
- Prompt-tuning latent diffusion models for inverse problems ([summary](https://blog.athina.ai/prompt-tuning-latent-diffusion-models-for-inverse-problems) | [paper](https://arxiv.org/abs/2310.01110))
- DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning ([summary](https://blog.athina.ai/dept-decomposed-prompt-tuning-for-parameter-efficient-fine-tuning) | [paper](https://arxiv.org/abs/2309.05173))
- EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM ([summary](https://blog.athina.ai/edgesam-prompt-in-the-loop-distillation-for-on-device-deployment-of-sam) | [paper](https://arxiv.org/abs/2312.06660))
- Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition ([summary](https://blog.athina.ai/prompt-tuning-of-deep-neural-networks-for-speaker-adaptive-visual-speech-recognition) | [paper](https://arxiv.org/abs/2302.08102))
- Visual Prompt Based Personalized Federated Learning ([summary](https://blog.athina.ai/visual-prompt-based-personalized-federated-learning) | [paper](https://arxiv.org/abs/2303.08678))
- SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks ([summary](https://blog.athina.ai/speechprompt-v2-prompt-tuning-for-speech-classification-tasks) | [paper](https://arxiv.org/abs/2303.00733))
- Privacy-Preserving Prompt Tuning for Large Language Model Services ([summary](https://blog.athina.ai/privacy-preserving-prompt-tuning-for-large-language-model-services) | [paper](https://arxiv.org/abs/2305.06212))
- IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models ([summary](https://blog.athina.ai/ip-adapter-text-compatible-image-prompt-adapter-for-text-to-image-diffusion-models) | [paper](https://arxiv.org/abs/2308.06721))
- Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game ([summary](https://blog.athina.ai/tensor-trust-interpretable-prompt-injection-attacks-from-an-online-game) | [paper](https://arxiv.org/abs/2311.01011))
- SGL-PT: A Strong Graph Learner with Graph Prompt Tuning ([summary](https://blog.athina.ai/sgl-pt-a-strong-graph-learner-with-graph-prompt-tuning) | [paper](https://arxiv.org/abs/2302.12449))
- The Flan Collection: Designing Data and Methods for Effective Instruction Tuning ([summary](https://blog.athina.ai/the-flan-collection-designing-data-and-methods-for-effective-instruction-tuning) | [paper](https://arxiv.org/abs/2301.13688))
- A-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting ([summary](https://blog.athina.ai/a-la-carte-prompt-tuning-apt-combining-distinct-data-via-composable-prompting) | [paper](https://arxiv.org/abs/2302.07994))
- How Does In-Context Learning Help Prompt Tuning? ([summary](https://blog.athina.ai/how-does-in-context-learning-help-prompt-tuning) | [paper](https://arxiv.org/abs/2302.11521))
- Effectiveness of Data Augmentation for Parameter Efficient Tuning with Limited Data ([summary](https://blog.athina.ai/effectiveness-of-data-augmentation-for-parameter-efficient-tuning-with-limited-data) | [paper](https://arxiv.org/abs/2303.02577))
- Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning ([summary](https://blog.athina.ai/multitask-prompt-tuning-enables-parameter-efficient-transfer-learning) | [paper](https://arxiv.org/abs/2303.02861))
- Visual-Language Prompt Tuning with Knowledge-guided Context Optimization ([summary](https://blog.athina.ai/visual-language-prompt-tuning-with-knowledge-guided-context-optimization) | [paper](https://arxiv.org/abs/2303.13283))
- Global Prompt Cell: A Portable Control Module for Effective Prompt Tuning ([summary](https://blog.athina.ai/global-prompt-cell-a-portable-control-module-for-effective-prompt-tuning) | [paper](https://arxiv.org/abs/2304.05642))
- Focused Prefix Tuning for Controllable Text Generation ([summary](https://blog.athina.ai/focused-prefix-tuning-for-controllable-text-generation) | [paper](https://arxiv.org/abs/2306.00369))
- Fine-tuning Language Models for Factuality ([summary](https://blog.athina.ai/fine-tuning-language-models-for-factuality) | [paper](https://arxiv.org/abs/2311.08401))
- Direct Preference Optimization: Your Language Model is Secretly a Reward Model ([summary](https://blog.athina.ai/direct-preference-optimization-your-language-model-is-secretly-a-reward-model) | [paper](https://arxiv.org/html/2305.18290v2))

## Evaluation 
- TopicGPT: A Prompt-based Topic Modeling Framework ([summary](https://blog.athina.ai/topicgpt-a-prompt-based-topic-modeling-framework) | [paper](https://arxiv.org/abs/2311.01449))
- Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization ([summary](https://blog.athina.ai/are-chatbots-ready-for-privacy-sensitive-applications-an-investigation-into-input-regurgitation-and-prompt-induced-sanitization) | [paper](https://arxiv.org/abs/2305.15008))
- ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation ([summary](https://blog.athina.ai/imagedream-image-prompt-multi-view-diffusion-for-3d-generation) | [paper](https://arxiv.org/abs/2312.02201))
- Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study ([summary](https://blog.athina.ai/jailbreaking-chatgpt-via-prompt-engineering-an-empirical-study) | [paper](https://arxiv.org/abs/2305.13860))
- An LLM can Fool Itself: A Prompt-Based Adversarial Attack ([summary](https://blog.athina.ai/an-llm-can-fool-itself-a-prompt-based-adversarial-attack) | [paper](https://arxiv.org/abs/2310.13345))
- Promptly: Using Prompt Problems to Teach Learners How to Effectively Utilize AI Code Generators ([summary](https://blog.athina.ai/promptly-using-prompt-problems-to-teach-learners-how-to-effectively-utilize-ai-code-generators) | [paper](https://arxiv.org/abs/2307.16364))
- PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models ([summary](https://blog.athina.ai/promptaid-prompt-exploration-perturbation-testing-and-iteration-using-visual-analytics-for-large-language-models) | [paper](https://arxiv.org/abs/2304.01964))
- Black-Box Prompt Optimization: Aligning Large Language Models without Model Training ([summary](https://blog.athina.ai/black-box-prompt-optimization-aligning-large-language-models-without-model-training) | [paper](https://arxiv.org/abs/2311.04155))
- State of What Art? A Call for Multi-Prompt LLM Evaluation ([summary](https://blog.athina.ai/state-of-what-art-a-call-for-multi-prompt-llm-evaluation) | [paper](https://arxiv.org/abs/2401.00595))
- Prompt Cache: Modular Attention Reuse for Low-Latency Inference ([summary](https://blog.athina.ai/prompt-cache-modular-attention-reuse-for-low-latency-inference) | [paper](https://arxiv.org/abs/2311.04934))
- Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential ([summary](https://blog.athina.ai/translating-radiology-reports-into-plain-language-using-chatgpt-and-gpt-4-with-prompt-learning-promising-results-limitations-and-potential) | [paper](https://arxiv.org/abs/2303.0))
- Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness ([summary](https://blog.athina.ai/dr-chatgpt-tell-me-what-i-want-to-hear-how-prompt-knowledge-impacts-health-answer-correctness) | [paper](https://arxiv.org/abs/2302.13793))
- Improving ChatGPT Prompt for Code Generation ([summary](https://blog.athina.ai/improving-chatgpt-prompt-for-code-generation) | [paper](https://arxiv.org/abs/2305.08360))
- SAMAug: Point Prompt Augmentation for Segment Anything Model ([summary](https://blog.athina.ai/samaug-point-prompt-augmentation-for-segment-anything-model) | [paper](https://arxiv.org/abs/2307.01187))
- A Novel Approach for Rapid Development Based on ChatGPT and Prompt Engineering ([summary](https://blog.athina.ai/a-novel-approach-for-rapid-development-based-on-chatgpt-and-prompt-engineering) | [paper](https://arxiv.org/abs/2312.13115))
- LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification ([summary](https://blog.athina.ai/lamper-language-model-and-prompt-engineering-for-zero-shot-time-series-classification) | [paper](https://arxiv.org/abs/2403.15875))
- A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering ([summary](https://blog.athina.ai/a-survey-on-segment-anything-model-sam-vision-foundation-model-meets-prompt-engineering) | [paper](https://arxiv.org/abs/2306.06211))
- Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering ([summary](https://blog.athina.ai/batch-calibration-rethinking-calibration-for-in-context-learning-and-prompt-engineering) | [paper](https://arxiv.org/abs/2309.17249))
- Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks ([summary](https://blog.athina.ai/prompt-engineering-or-fine-tuning-an-empirical-assessment-of-large-language-models-in-automated-software-engineering-tasks) | [paper](https://arxiv.org/abs/2310.10508))
- Design Guidelines for Prompt Engineering Text-to-Image Generative Models ([summary](https://blog.athina.ai/researdesign-guidelines-for-prompt-engineering-text-to-image-generative-models) | [paper](https://arxiv.org/abs/2109.06977))
- Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences ([summary](https://blog.athina.ai/who-validates-the-validators-aligning-llm-assisted-evaluation-of-llm-outputs-with-human-preferences) | [paper](https://arxiv.org/abs/2404.12272))
- Automatic Root Cause Analysis via Large Language Models for Cloud Incidents ([summary](https://blog.athina.ai/automatic-root-cause-analysis-via-large-language-models-for-cloud-incidents) | [paper](https://arxiv.org/abs/2305.15778))
- Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines ([summary](https://blog.athina.ai/enhancing-large-language-models-for-clinical-decision-support-by-incorporating-clinical-practice-guidelines) | [paper](https://arxiv.org/abs/2401.11120))
- NLPBench: Evaluating Large Language Models on Solving NLP Problems ([summary](https://blog.athina.ai/nlpbench-evaluating-large-language-models-on-solving-nlp-problems) | [paper](https://arxiv.org/abs/2309.15630))
- Founder-GPT: Self-play to evaluate the Founder-Idea fit ([summary](https://blog.athina.ai/founder-gpt-self-play-to-evaluate-the-founder-idea-fit) | [paper](https://arxiv.org/abs/2312.12037))
- Successive Prompting for Decomposing Complex Questions ([summary](https://blog.athina.ai/successive-prompting-for-decomposing-complex-questions) | [paper](https://arxiv.org/abs/2212.04092))
- Batch Prompting: Efficient Inference with Large Language Model APIs ([summary](https://blog.athina.ai/batch-prompting-efficient-inference-with-large-language-model-apis) | [paper](https://arxiv.org/abs/2301.08721))
- Progressive Prompts: Continual Learning for Language Models ([summary](https://blog.athina.ai/progressive-prompts-continual-learning-for-language-models) | [paper](https://arxiv.org/abs/2301.12314))
- Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models ([summary](https://blog.athina.ai/synthetic-prompting-generating-chain-of-thought-demonstrations-for-large-language-models) | [paper](https://arxiv.org/abs/2302.00618))
- Large Language Models Can Be Easily Distracted by Irrelevant Context ([summary](https://blog.athina.ai/large-language-models-can-be-easily-distracted-by-irrelevant-context) | [paper](https://arxiv.org/abs/2302.00093))
- Evaluating the Robustness of Discrete Prompts ([summary](https://blog.athina.ai/evaluating-the-robustness-of-discrete-prompts) | [paper](https://arxiv.org/abs/2302.05619))
- Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints ([summary](https://blog.athina.ai/bounding-the-capabilities-of-large-language-models-in-open-text-generation-with-prompt-constraints) | [paper](https://arxiv.org/abs/2302.09185))
- Active Prompting with Chain-of-Thought for Large Language Models ([summary](https://blog.athina.ai/active-prompting-with-chain-of-thought-for-large-language-models) | [paper](https://arxiv.org/abs/2302.12246))
- Chain of Hindsight Aligns Language Models with Feedback ([summary](https://blog.athina.ai/chain-of-hindsight-aligns-language-models-with-feedback) | [paper](https://arxiv.org/abs/2302.02676))
- Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT ([summary](https://blog.athina.ai/can-chatgpt-understand-too-a-comparative-study-on-chatgpt-and-fine-tuned-bert) | [paper](https://arxiv.org/pdf/2302.10198))
- How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks ([summary](https://blog.athina.ai/how-robust-is-gpt-3.5-to-predecessors-a-comprehensive-study-on-language-understanding-tasks) | [paper](https://arxiv.org/abs/2303.00293))
- OpenICL: An Open-Source Framework for In-context Learning ([summary](https://blog.athina.ai/openicl-an-open-source-framework-for-in-context-learning) | [paper](https://arxiv.org/abs/2303.02913))
- Larger language models do in-context learning differently ([summary](https://blog.athina.ai/larger-language-models-do-in-context-learning-differently) | [paper](https://arxiv.org/abs/2303.03846))
- CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification ([summary](https://blog.athina.ai/cotever-chain-of-thought-prompting-annotation-toolkit-for-explanation-verification) | [paper](https://arxiv.org/abs/2303.03628))
- Context-faithful Prompting for Large Language Models ([summary](https://blog.athina.ai/context-faithful-prompting-for-large-language-models) | [paper](https://arxiv.org/abs/2303.11315))
- Fairness-guided Few-shot Prompting for Large Language Models ([summary](https://blog.athina.ai/fairness-guided-few-shot-prompting-for-large-language-models) | [paper](https://arxiv.org/abs/2303.13217))
- NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference ([summary](https://blog.athina.ai/nn-prompting-beyond-context-learning-with-calibration-free-nearest-neighbor-inference) | [paper](https://arxiv.org/abs/2303.13824))
- Self-Refine: Iterative Refinement with Self-Feedback ([summary](https://blog.athina.ai/self-refine-iterative-refinement-with-self-feedback) | [paper](https://arxiv.org/abs/2303.17651v1))
- Reflexion: Language Agents with Verbal Reinforcement Learning ([summary](https://blog.athina.ai/reflexion-language-agents-with-verbal-reinforcement-learning) | [paper](https://arxiv.org/abs/2303.11366))
- Revisiting Automated Prompting: Are We Actually Doing Better? ([summary](https://blog.athina.ai/revisiting-automated-prompting-are-we-actually-doing-better) | [paper](https://arxiv.org/abs/2304.03609))
- Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models ([summary](https://blog.athina.ai/chain-of-symbol-prompting-elicits-planning-in-large-langauge-models) | [paper](https://arxiv.org/abs/2305.10276))
- ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs ([summary](https://blog.athina.ai/zeroprompt-streaming-acoustic-encoders-are-zero-shot-masked-lms) | [paper](https://arxiv.org/abs/2305.10649))
- Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency ([summary](https://blog.athina.ai/flatness-aware-prompt-selection-improves-accuracy-and-sample-efficiency) | [paper](https://arxiv.org/abs/2305.10713))
- TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks ([summary](https://blog.athina.ai/teler-a-general-taxonomy-of-llm-prompts-for-benchmarking-complex-tasks) | [paper](https://arxiv.org/abs/2305.11430))
- Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs ([summary](https://blog.athina.ai/let-s-sample-step-by-step-adaptive-consistency-for-efficient-reasoning-and-coding-with-llms) | [paper](https://arxiv.org/abs/2305.11860))
- Interactive Natural Language Processing ([summary](https://blog.athina.ai/interactive-natural-language-processing) | [paper](https://arxiv.org/abs/2305.13246))
- Let's Verify Step by Step ([summary](https://blog.athina.ai/let-s-verify-step-by-step) | [paper](https://arxiv.org/abs/2305.20050))
- Temporal evolution of depolarization and magnetic field of FRB 20201124A ([summary](https://blog.athina.ai/temporal-evolution-of-depolarization-and-magnetic-field-of-frb-20201124a) | [paper](https://arxiv.org/abs/2309.06653))
- A Survey on In-context Learning ([summary](https://blog.athina.ai/a-survey-on-in-context-learning) | [paper](https://arxiv.org/abs/2301.00234))
- A Bibliometric Review of Large Language Models Research from 2017 to 2023 ([summary](https://blog.athina.ai/a-bibliometric-review-of-large-language-models-research-from-2017-to-2023) | [paper](https://arxiv.org/abs/2304.02020))
- Tool Learning with Foundation Models ([summary](https://blog.athina.ai/tool-learning-with-foundation-models) | [paper](https://arxiv.org/abs/2304.08354))
- Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond ([summary](https://blog.athina.ai/harnessing-the-power-of-llms-in-practice-a-survey-on-chatgpt-and-beyond) | [paper](https://arxiv.org/abs/2304.13712))
- Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation ([summary](https://blog.athina.ai/few-shot-fine-tuning-vs.-in-context-learning-a-fair-comparison-and-evaluation) | [paper](https://arxiv.org/abs/2305.16938))

## Safety
- Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information ([summary](https://blog.athina.ai/token-level-adversarial-prompt-detection-based-on-perplexity-measures-and-contextual-information) | [paper](https://arxiv.org/abs/2311.11509))
- DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer ([summary](https://blog.athina.ai/dp-opt-make-large-language-model-your-privacy-preserving-prompt-engineer) | [paper](https://arxiv.org/abs/2312.03724))
- Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration ([summary](https://blog.athina.ai/practical-membership-inference-attacks-against-fine-tuned-large-language-models-via-self-prompt-calibration) | [paper](https://arxiv.org/abs/2311.06062))
- Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models ([summary](https://blog.athina.ai/benchmarking-and-defending-against-indirect-prompt-injection-attacks-on-large-language-models) | [paper](https://arxiv.org/abs/2312.14197))
- Assessing Prompt Injection Risks in 200+ Custom GPTs ([summary](https://blog.athina.ai/assessing-prompt-injection-risks-in-200-custom-gpts) | [paper](https://arxiv.org/abs/2311.11538))
- Prompt Stealing Attacks Against Text-to-Image Generation Models ([summary](https://blog.athina.ai/prompt-stealing-attacks-against-text-to-image-generation-models) | [paper](https://arxiv.org/abs/2302.09923))
- PromptCARE: Prompt Copyright Protection by Watermark Injection and Verification ([summary](https://blog.athina.ai/promptcare-prompt-copyright-protection-by-watermark-injection-and-verification) | [paper](https://arxiv.org/abs/2308.02816))
- LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers ([summary](https://blog.athina.ai/llms-can-understand-encrypted-prompt-towards-privacy-computing-friendly-transformers) | [paper](https://arxiv.org/abs/2305.18396))
- Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks ([summary](https://blog.athina.ai/prompt-packer-deceiving-llms-through-compositional-instruction-with-hidden-attacks) | [paper](https://arxiv.org/abs/2310.10077))
- From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application? ([summary](https://blog.athina.ai/from-prompt-injections-to-sql-injection-attacks-how-protected-is-your-llm-integrated-web-application) | [paper](https://arxiv.org/abs/2308.01990))
- Prompt Injection attack against LLM-integrated Applications ([summary](https://blog.athina.ai/prompt-injection-attack-against-llm-integrated-applications) | [paper](https://arxiv.org/abs/2306.05499))
- Prompting GPT-3 To Be Reliable ([summary](https://blog.athina.ai/prompting-gpt-3-to-be-reliable) | [paper](https://arxiv.org/abs/2210.09150))
- Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods ([summary](https://blog.athina.ai/machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methods) | [paper](https://arxiv.org/abs/2210.07321))
- Tree of Attacks: Jailbreaking Black-Box LLMs Automatically ([summary](https://blog.athina.ai/tree-of-attacks-jailbreaking-black-box-llms-automatically) | [paper](https://arxiv.org/abs/2312.02119))
- On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning ([summary](https://blog.athina.ai/on-second-thought-let-s-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoning) | [paper](https://arxiv.org/abs/2212.08061))
- Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection ([summary](https://blog.athina.ai/not-what-you-ve-signed-up-for-compromising-real-world-llm-integrated-applications-with-indirect-prompt-injection) | [paper](https://arxiv.org/abs/2302.12173))
- CYBERSECEVAL 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models ([summary](https://blog.athina.ai/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models) | [paper](https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/))
- Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification ([summary](https://blog.athina.ai/ever-mitigating-hallucination-in-large-language-models-through-real-time-verification-and-rectification) | [paper](https://arxiv.org/html/2311.09114v2))
- Universal and Transferable Adversarial Attacks on Aligned Language Models ([summary](https://blog.athina.ai/universal-and-transferable-adversarial-attacks-on-aligned-language-models) | [paper](https://arxiv.org/abs/2307.15043))
- From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings ([summary](https://blog.athina.ai/from-noise-to-clarity-unraveling-the-adversarial-suffix-of-large-language-model-attacks-via-translation-of-text-embeddings) | [paper](https://arxiv.org/abs/2402.16006))
- Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models ([summary](https://blog.athina.ai/breaking-down-the-defenses-a-comparative-survey-of-attacks-on-large-language-models) | [paper](https://arxiv.org/pdf/2403.04786.pdf))
- Many-Shot Jailbreaking (Anthropic Research) ([summary](https://blog.athina.ai/many-shot-jailbreaking-anthropic-research) | [paper](https://www.anthropic.com/research/many-shot-jailbreaking))
- AI Safety: Necessary, but insufficient and possibly problematic ([summary](https://blog.athina.ai/ai-safety-necessary-but-insufficient-and-possibly-problematic) | [paper](https://arxiv.org/html/2403.17419v1))
